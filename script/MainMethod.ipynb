{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Dataset/final_questions_only.json\", \"r\") as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the model\n",
    "# NOTE: CodeLlama requires license. You must obtain one thorugh Meta and registered it in your huggingface account.\n",
    "# To utilize CodeLlama as the paper has done, before running the code below, please run the following command:\n",
    "# from huggingface_hub import login\n",
    "# login(token = \"Your_HuggingFace_Token\")\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "model_name = \"meta-llama/CodeLlama-13b-Instruct-hf\"\n",
    "\n",
    "# Load the tokenizer and model.\n",
    "cache_custom_dir = \"/data/gpfs/projects/punim2402/huggingface\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = cache_custom_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir = cache_custom_dir, device_map=\"cuda\")\n",
    "\n",
    "# set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for the Log Prob Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for the log prob\n",
    "def compute_log_prob(context_question: str, reference_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the total log probability of the expected_answer given the question.\n",
    "    \"\"\"\n",
    "    format_prompt = \"\"\"Start your answer with Student: \"\"\"\n",
    "\n",
    "    context_prompt = \"\"\"\n",
    "            You are a first-year programming student who is learning how to debug your Python code. You are in the middle of a tutoring session.\n",
    "            You are given a word problem task, a code that you have submitted with a bug, and a conversation history that you have been having with your tutor.\n",
    "            Your task is to respond to the latest question asked by your teacher in the dialogue so far.\n",
    "            Your goal is to find a solution to the bug code through Socratic dialogue.\n",
    "\n",
    "            Respond like a real student: \n",
    "            - Think out loud and explain your reasoning.\n",
    "            - Only respond to the most recent question.\n",
    "            - Use the code you've been working on to guide your answer.\n",
    "        \"\"\"\n",
    "    \n",
    "    sysprompt = format_prompt + context_prompt\n",
    "    # Tokenize the question and answer.\n",
    "    messages_with_answer = [\n",
    "        {\"role\": \"system\", \"content\": sysprompt},\n",
    "        {\"role\": \"user\", \"content\": context_question},\n",
    "        {\"role\": \"assistant\", \"content\": \"Student: \"  + reference_answer}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages_with_answer, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Extract the user message length\n",
    "    messages_to_last_utterance = [\n",
    "        {\"role\": \"system\", \"content\": sysprompt},\n",
    "        {\"role\": \"user\", \"content\": context_question}\n",
    "    ]\n",
    "    input_ids_up_to_user = tokenizer.apply_chat_template(messages_to_last_utterance, return_tensors=\"pt\")\n",
    "    user_length = input_ids_up_to_user.shape[1]\n",
    "\n",
    "    # Extract the answer token ids\n",
    "    answer_token_ids = input_ids[:, user_length:]\n",
    "    answer_length = answer_token_ids.shape[1]\n",
    "    \n",
    "    # Calculate the log probabilities of the answer tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Slice logits to get those corresponding to the answer tokens\n",
    "    logits_for_answer = logits[:, user_length - 1 : user_length + answer_length - 1, :]\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits_for_answer, dim=-1)\n",
    "    token_log_probs = log_probs.gather(2, answer_token_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    total_log_prob = token_log_probs.sum().item()\n",
    "    \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the calculation for each question and solution pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the log probability for each pair.\n",
    "results = {}\n",
    "for filename, value in content[0].items():\n",
    "    print(filename)\n",
    "    results[filename] = {}\n",
    "    reference_answer = value['solution']\n",
    "\n",
    "    # Retrieve the levels\n",
    "    levels = list(value.keys())\n",
    "    levels.remove('solution')\n",
    "    \n",
    "    for level in levels:\n",
    "        context_question = value[level]\n",
    "        log_prob = compute_log_prob(context_question, reference_answer)\n",
    "        results[filename][level] = log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name accordingly to the model used\n",
    "with open(\"../Result/log_prob_score_final_question_codeLlama.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuVLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
