{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"full_question_solution_updated.json\", \"r\") as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c005c375bb4b028f438bcdd81cc560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "# Load the tokenizer and model.\n",
    "cache_custom_dir = \"/data/gpfs/projects/punim2402/huggingface\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = cache_custom_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir = cache_custom_dir, device_map=\"cuda\")\n",
    "model.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for the Log Prob Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for the log prob\n",
    "def compute_log_prob(context_question: str, reference_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the total log probability of the expected_answer given the question.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a beginner programming student. Based on your buggy code, you are having a conversation with a teacher \n",
    "    that is helping you solve the problem. The teacher is a Socratic Tutor that is leading you to the correct answer with varying level\n",
    "    of question directness. Based on the final question, do your best to think of the solution to the problem. Provide a response to the \n",
    "    final question from the teacher.\"\"\"\n",
    "\n",
    "    context = \"\"\"You are a beginner programming student, and you are trying to debug a piece of code. You are having a conversation with a teacher who is guiding you \n",
    "    through the debugging process. The teacher follows the Socratic method, asking questions that lead you to the correct solution. \n",
    "    Your teacher's questions vary in directness, ranging from questions that directly point out the error in your code to more exploratory, \n",
    "    indirect questions that help you think about concepts or related issues.\n",
    "    Based on the final question from the teacher, think about the solution to the problem and provide a response to the question.\n",
    "    Here are the guidelines:\n",
    "    - A **direct question** is specific and asks for a clear, concrete answer (e.g., \"What is wrong with line 3?\").\n",
    "    - An **indirect question** is more general or conceptual and may require you to think more broadly about the issue (e.g., \"What do you understand about syntax errors?\").\n",
    "    The goal is to evaluate how the level of **directness** in the teacher's question affects your ability and confidence to identify the solution to the problem.\"\"\"\n",
    "    \n",
    "    # Tokenize the question and answer.\n",
    "    messages_with_answer = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": context + context_question},\n",
    "        {\"role\": \"assistant\", \"content\": \"Student: \"  + reference_answer}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages_with_answer, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    messages_to_last_utterance = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": context + context_question}\n",
    "    ]\n",
    "    input_ids_up_to_user = tokenizer.apply_chat_template(messages_to_last_utterance, return_tensors=\"pt\")\n",
    "    user_length = input_ids_up_to_user.shape[1]\n",
    "\n",
    "    # Extract the answer token ids\n",
    "    answer_token_ids = input_ids[:, user_length:]\n",
    "    answer_length = answer_token_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Slice logits to get those corresponding to the answer tokens\n",
    "    logits_for_answer = logits[:, user_length - 1 : user_length + answer_length - 1, :]\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits_for_answer, dim=-1)\n",
    "    token_log_probs = log_probs.gather(2, answer_token_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    total_log_prob = token_log_probs.sum().item()\n",
    "    \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the calculation for each question and solution pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1_fibonacci\n",
      "0_2_fibonacci\n",
      "0_5_fibonacci\n",
      "0_6_fibonacci\n",
      "10_39_xnglxsh\n",
      "11_40_palindrome\n",
      "12_41_reversing_a_list\n",
      "13_42_limit\n",
      "14_43_used_twice\n",
      "15_44_sequential_search\n",
      "15_45_sequential_search\n",
      "16_46_substring_length\n",
      "16_56_substring_length\n",
      "17_47_topk_socratic\n",
      "18_48_password_validator\n",
      "19_49_word_counter\n",
      "19_50_word_counter\n",
      "1_10_calculating_a_grade\n",
      "1_11_calculating_a_grade\n",
      "1_13_calculating _a_grade\n",
      "1_8_calculating_a_grade\n",
      "1_9_calculating_a_grade\n",
      "20_51_spell_checker\n",
      "21_52_fahrenheit_to_celsius\n",
      "22_53_cookie_purchase\n",
      "24_29_factorial_socratic\n",
      "25_55_insert_to_linked_list\n",
      "2_18_splitting_cookies\n",
      "3_20_counting_down\n",
      "4_22_removing_even_number\n",
      "4_23_removing_even_numer\n",
      "4_25_removing_even_numer\n",
      "4_26_removing_even_numbers\n",
      "4_28_removing_even_numbers\n",
      "56_15_compute_average\n",
      "58_58_splitting_apples\n",
      "58_59_splitting_apples\n",
      "59_60_product\n",
      "5_30_sorted_words\n",
      "60_61_largest_number\n",
      "61_62_is_even\n",
      "62_63_summing_between_integers\n",
      "63_64_good_dinner\n",
      "64_65_count_ones\n",
      "65_66_list_range\n",
      "66_67_last_index_of\n",
      "66_68_last_index_of\n",
      "66_69_last_index_of\n",
      "67_70_area_circle\n",
      "69_81_get_combinations\n",
      "6_33_turning_clockwise\n",
      "6_34_turning_clockwise\n",
      "74_77_disney_vacation_club\n",
      "74_78_disney_vacation_club\n",
      "7_35_integer_grouping\n",
      "9_38_calculating_determinant\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the log probability for each pair.\n",
    "results = {}\n",
    "for filename, value in content.items():\n",
    "    print(filename)\n",
    "    results[filename] = {}\n",
    "    reference_answer = value['solution']\n",
    "\n",
    "    # Retrieve the levels\n",
    "    levels = list(value.keys())\n",
    "    levels.remove('solution')\n",
    "    \n",
    "    for level in levels:\n",
    "        context_question = value[level]\n",
    "        log_prob = compute_log_prob(context_question, reference_answer)\n",
    "        results[filename][level] = log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"log_prob_score_full_question_testing.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0_1_fibonacci': {'least': -152.23660278320312, 'more': -143.716796875, 'most': -150.20327758789062}, '0_2_fibonacci': {'least': -131.60476684570312, 'more': -148.72409057617188, 'most': -148.0623779296875}, '0_5_fibonacci': {'least': -125.0540542602539, 'more': -120.92655181884766, 'most': -125.02173614501953}, '0_6_fibonacci': {'least': -130.5362091064453, 'more': -132.58868408203125, 'most': -142.78533935546875}, '10_39_xnglxsh': {'least': -155.59913635253906, 'more': -150.81544494628906, 'most': -156.98348999023438}, '11_40_palindrome': {'least': -102.663818359375, 'more': -96.79672241210938, 'most': -95.99673461914062}, '12_41_reversing_a_list': {'least': -129.89659118652344, 'more': -136.08409118652344, 'most': -133.07806396484375}, '13_42_limit': {'least': -135.551025390625, 'more': -129.29991149902344, 'most': -131.4855499267578}, '14_43_used_twice': {'least': -101.49087524414062, 'more': -109.9072036743164, 'most': -104.3679428100586}, '15_44_sequential_search': {'least': -133.97842407226562, 'more': -158.8948974609375, 'most': -140.64370727539062}, '15_45_sequential_search': {'least': -174.68359375, 'more': -170.66717529296875, 'most': -167.772705078125}, '16_46_substring_length': {'least': -189.25265502929688, 'more': -186.86495971679688, 'most': -178.30572509765625}, '16_56_substring_length': {'least': -196.7249755859375, 'more': -194.32476806640625, 'most': -196.5885009765625}, '17_47_topk_socratic': {'least': -163.15475463867188, 'more': -164.47329711914062, 'most': -154.02694702148438}, '18_48_password_validator': {'least': -136.56182861328125, 'more': -139.0867462158203, 'most': -129.596923828125}, '19_49_word_counter': {'least': -173.1680450439453, 'more': -176.25955200195312, 'most': -170.28335571289062}, '19_50_word_counter': {'least': -165.43374633789062, 'more': -158.137451171875, 'most': -157.43394470214844}, '1_10_calculating_a_grade': {'least': -188.42800903320312, 'more': -180.87200927734375, 'most': -184.91693115234375}, '1_11_calculating_a_grade': {'least': -164.087646484375, 'more': -167.67221069335938, 'most': -163.97935485839844}, '1_13_calculating _a_grade': {'least': -168.9807891845703, 'more': -171.72488403320312, 'most': -172.98458862304688}, '1_8_calculating_a_grade': {'least': -146.0342559814453, 'more': -141.5802764892578, 'most': -140.274169921875}, '1_9_calculating_a_grade': {'least': -172.39352416992188, 'most': -170.71243286132812}, '20_51_spell_checker': {'least': -117.1021728515625, 'more': -103.90390014648438, 'most': -111.17446899414062}, '21_52_fahrenheit_to_celsius': {'least': -105.95321655273438, 'more': -123.75005340576172, 'most': -116.02217864990234}, '22_53_cookie_purchase': {'least': -163.1503448486328, 'more': -146.872802734375, 'most': -146.9525146484375}, '24_29_factorial_socratic': {'least': -225.64346313476562, 'most': -224.66856384277344}, '25_55_insert_to_linked_list': {'least': -172.93978881835938, 'more': -171.152099609375, 'most': -182.69613647460938}, '2_18_splitting_cookies': {'least': -346.1376647949219, 'more': -340.15850830078125, 'most': -337.8433837890625}, '3_20_counting_down': {'least': -141.60325622558594, 'more': -148.29733276367188, 'most': -152.43740844726562}, '4_22_removing_even_number': {'least': -98.7955551147461, 'more': -93.19676208496094, 'most': -81.0766372680664}, '4_23_removing_even_numer': {'least': -185.21621704101562, 'more': -191.8070068359375, 'most': -180.72836303710938}, '4_25_removing_even_numer': {'least': -109.57618713378906, 'most': -102.79193878173828}, '4_26_removing_even_numbers': {'least': -184.2184600830078, 'more': -176.35067749023438, 'most': -173.51727294921875}, '4_28_removing_even_numbers': {'least': -88.92540740966797, 'more': -84.98646545410156, 'most': -83.18394470214844}, '56_15_compute_average': {'least': -185.86703491210938, 'more': -190.74874877929688, 'most': -181.4197998046875}, '58_58_splitting_apples': {'least': -99.60059356689453, 'more': -106.59024047851562, 'most': -101.50704193115234}, '58_59_splitting_apples': {'least': -135.33831787109375, 'more': -134.81715393066406, 'most': -114.46013641357422}, '59_60_product': {'least': -119.43304443359375, 'more': -117.93966674804688, 'most': -125.60377502441406}, '5_30_sorted_words': {'least': -227.4676971435547, 'more': -214.480712890625, 'most': -221.12525939941406}, '60_61_largest_number': {'least': -135.41331481933594, 'more': -134.2157745361328, 'most': -110.35272216796875}, '61_62_is_even': {'least': -113.49285125732422, 'more': -124.4572982788086, 'most': -111.51899719238281}, '62_63_summing_between_integers': {'least': -111.81043243408203, 'more': -112.43901062011719, 'most': -109.66310119628906}, '63_64_good_dinner': {'least': -124.48336029052734, 'more': -117.43545532226562, 'most': -115.85606384277344}, '64_65_count_ones': {'least': -179.47525024414062, 'more': -165.2815704345703, 'most': -180.76803588867188}, '65_66_list_range': {'least': -139.70272827148438, 'more': -145.1763916015625, 'most': -143.398681640625}, '66_67_last_index_of': {'least': -94.72550964355469, 'more': -96.65874481201172, 'most': -90.38230895996094}, '66_68_last_index_of': {'least': -168.72854614257812, 'more': -166.54425048828125, 'most': -168.778564453125}, '66_69_last_index_of': {'least': -162.22262573242188, 'more': -173.70448303222656, 'most': -173.70448303222656}, '67_70_area_circle': {'least': -177.25946044921875, 'more': -155.51513671875, 'most': -166.66070556640625}, '69_81_get_combinations': {'least': -140.29519653320312, 'more': -134.60833740234375, 'most': -129.30230712890625}, '6_33_turning_clockwise': {'least': -186.2762451171875, 'more': -159.0810089111328, 'most': -167.94155883789062}, '6_34_turning_clockwise': {'least': -88.66787719726562, 'more': -76.0757064819336, 'most': -81.00181579589844}, '74_77_disney_vacation_club': {'least': -167.1788787841797, 'more': -165.75668334960938, 'most': -156.04071044921875}, '74_78_disney_vacation_club': {'least': -138.80392456054688, 'more': -136.6974639892578, 'most': -127.66401672363281}, '7_35_integer_grouping': {'least': -198.60618591308594, 'more': -194.71994018554688, 'most': -194.71994018554688}, '9_38_calculating_determinant': {'least': -177.2899627685547, 'more': -169.102783203125, 'most': -169.21722412109375}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
